{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1dc0af7-8de4-46e3-8aa6-de15318b58cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.key.tokyoolympicdata99.dfs.core.windows.net\",\"cdXZOr2G1ZwwvTAMKDDUKBhFht6lR8TJbfUfvpecw9NntXlocw1FJlhWE2cd+ZgQuAE6+d/ZlHnb+AStlnfOZQ==\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e0f489-8929-40fc-b444-285537c1f403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook 1: Data Quality Checks\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, lit, expr\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Data Quality Checks\").getOrCreate()\n",
    "\n",
    "# Define paths for datasets\n",
    "athletes_path = \"abfss://tokyo-olympic-data@tokyoolympicdata99.dfs.core.windows.net/raw-data/Athletes.csv\"\n",
    "coaches_path = \"abfss://tokyo-olympic-data@tokyoolympicdata99.dfs.core.windows.net/raw-data/Coaches.csv\"\n",
    "entriesgender_path = \"abfss://tokyo-olympic-data@tokyoolympicdata99.dfs.core.windows.net/raw-data/EntriesGender.csv\"\n",
    "medals_path = \"abfss://tokyo-olympic-data@tokyoolympicdata99.dfs.core.windows.net/raw-data/Medals.csv\"\n",
    "teams_path = \"abfss://tokyo-olympic-data@tokyoolympicdata99.dfs.core.windows.net/raw-data/Teams.csv\"\n",
    "\n",
    "# Load datasets\n",
    "athletes_df = spark.read.csv(athletes_path, header=True, inferSchema=True)\n",
    "coaches_df = spark.read.csv(coaches_path, header=True, inferSchema=True)\n",
    "entriesgender_df = spark.read.csv(entriesgender_path, header=True, inferSchema=True)\n",
    "medals_df = spark.read.csv(medals_path, header=True, inferSchema=True)\n",
    "teams_df = spark.read.csv(teams_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f8f218-6dcc-4e54-b8ae-48a872f8a2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n|Name|NOC|Discipline|\n+----+---+----------+\n|   0|  0|         0|\n+----+---+----------+\n\n+----+---+----------+-----+\n|Name|NOC|Discipline|Event|\n+----+---+----------+-----+\n|   0|  0|         0|  145|\n+----+---+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Null Checks\n",
    "def perform_null_checks(df, df_name):\n",
    "    null_counts = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns])\n",
    "    null_counts.show()\n",
    "    return null_counts\n",
    "\n",
    "null_athletes = perform_null_checks(athletes_df, \"athletes\")\n",
    "null_coaches = perform_null_checks(coaches_df, \"coaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989406d7-d7dc-417a-8f55-04db08a64a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-----+\n|       Name|    NOC|Discipline|count|\n+-----------+-------+----------+-----+\n|ALI Mohamed|Bahrain|  Handball|    2|\n+-----------+-------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Duplicate Checks\n",
    "def perform_duplicate_checks(df, df_name, subset_cols):\n",
    "    duplicate_count = df.groupBy(subset_cols).count().filter(col(\"count\") > 1)\n",
    "    duplicate_count.show()\n",
    "    return duplicate_count\n",
    "\n",
    "duplicate_athletes = perform_duplicate_checks(athletes_df, \"athletes\", [\"Name\", \"NOC\", \"Discipline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955e478d-dc3d-4041-81bd-4dbf186705b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-----+\n|Discipline|Female|Male|Total|\n+----------+------+----+-----+\n+----------+------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Range Checks\n",
    "def perform_range_checks(df, column, min_val, max_val, df_name):\n",
    "    invalid_range = df.filter((col(column) < min_val) | (col(column) > max_val))\n",
    "    invalid_range.show()\n",
    "    return invalid_range\n",
    "\n",
    "range_entriesgender = perform_range_checks(entriesgender_df, \"Total\", 0, 5000, \"entriesgender\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4ceedb-3840-417a-b36c-9d838319cb14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+------+------+-----+-------------+\n|Rank|Team/NOC|Gold|Silver|Bronze|Total|Rank by Total|\n+----+--------+----+------+------+-----+-------------+\n+----+--------+----+------+------+-----+-------------+\n\nNo invalid medal counts found.\n"
     ]
    }
   ],
   "source": [
    "def validate_medal_counts(df):\n",
    "    # Check for negative values in Gold, Silver, and Bronze columns\n",
    "    invalid_medals = df.filter((col(\"Gold\") < 0) | (col(\"Silver\") < 0) | (col(\"Bronze\") < 0))\n",
    "    invalid_medals.show()\n",
    "    return invalid_medals\n",
    "\n",
    "\n",
    "# Validate medal counts in the medals dataset\n",
    "invalid_medals = validate_medal_counts(medals_df)\n",
    "\n",
    "# Handle the invalid_medals DataFrame if needed\n",
    "if invalid_medals.count() > 0:\n",
    "    print(f\"Found {invalid_medals.count()} rows with invalid medal counts.\")\n",
    "else:\n",
    "    print(\"No invalid medal counts found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c8a28c-a3a3-4143-b6af-e4dd6e65d8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+------+------+-----+-------------+\n|Rank|Team/NOC|Gold|Silver|Bronze|Total|Rank by Total|\n+----+--------+----+------+------+-----+-------------+\n+----+--------+----+------+------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Negative Values Check\n",
    "def check_negative_values(df, column, df_name):\n",
    "    negative_values = df.filter(col(column) < 0)\n",
    "    negative_values.show()\n",
    "    return negative_values\n",
    "\n",
    "negative_gold = check_negative_values(medals_df, \"Gold\", \"medals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374de3ba-0cb6-4484-9375-9dcb6cdb349f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-----+\n|Discipline|Female|Male|Total|\n+----------+------+----+-----+\n+----------+------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Data Type Validation\n",
    "def perform_data_type_validation(df, column, expected_type, df_name):\n",
    "    invalid_data_type = df.filter(~col(column).cast(expected_type).isNotNull())\n",
    "    invalid_data_type.show()\n",
    "    return invalid_data_type\n",
    "\n",
    "invalid_entriesgender_total = perform_data_type_validation(entriesgender_df, \"Total\", \"int\", \"entriesgender\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed78b97-a45e-4ccb-af44-81675e4ef671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+\n|                Name|                 NOC|   Discipline|\n+--------------------+--------------------+-------------+\n|       ABASS Abobakr|               Sudan|     Swimming|\n| ABDALRASOOL Mohamed|               Sudan|         Judo|\n|ABDUL RAZZAQ Fath...|            Maldives|    Badminton|\n|       ABEBE Mekides|            Ethiopia|    Athletics|\n|       ABELA Matthew|               Malta|    Badminton|\n|  ABEYSINGHE Matthew|           Sri Lanka|     Swimming|\n|     ABIDINE Abidine|          Mauritania|    Athletics|\n|         ABOUD Hadel|               Libya|    Athletics|\n| ABOUKE Nancy Genzel|               Nauru|Weightlifting|\n|       ABRAMS Aliyah|              Guyana|    Athletics|\n|      ABRAMS Jasmine|              Guyana|    Athletics|\n|      ABRAMYAN Benik|             Georgia|    Athletics|\n|      ABU RABEE Asma|              Jordan|     Shooting|\n|    ABU RMILAH Wesam|           Palestine|         Judo|\n|  ACHILLEOS Georgios|              Cyprus|     Shooting|\n|      ACOSTA Marcelo|         El Salvador|     Swimming|\n|      ADAMS Taeyanna|Federated States ...|     Swimming|\n|AFOUMBA Gilles An...|               Congo|    Athletics|\n|    AFRIAT Charlotte|              Monaco|    Athletics|\n|  AGAHOZO Alphonsine|              Rwanda|     Swimming|\n+--------------------+--------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Referential Integrity\n",
    "def check_referential_integrity(parent_df, child_df, parent_col, child_col, df_name):\n",
    "    missing_references = child_df.join(parent_df, parent_df[parent_col] == child_df[child_col], \"left_anti\")\n",
    "    missing_references.show()\n",
    "    return missing_references\n",
    "\n",
    "missing_noc_references = check_referential_integrity(teams_df, athletes_df, \"NOC\", \"NOC\", \"athletes_teams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7ca61a-15d5-401f-b1bc-b5716e4b1c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n|                Name|                 NOC|         Discipline|\n+--------------------+--------------------+-------------------+\n|     AALERUD Katrine|              Norway|       Cycling Road|\n|         ABAD Nestor|               Spain|Artistic Gymnastics|\n|   ABAGNALE Giovanni|               Italy|             Rowing|\n|      ABALDE Alberto|               Spain|         Basketball|\n|       ABALDE Tamara|               Spain|         Basketball|\n|           ABALO Luc|              France|           Handball|\n|        ABAROA Cesar|               Chile|             Rowing|\n|       ABASS Abobakr|               Sudan|           Swimming|\n|    ABBASALI Hamideh|Islamic Republic ...|             Karate|\n|       ABBASOV Islam|          Azerbaijan|          Wrestling|\n|        ABBINGH Lois|         Netherlands|           Handball|\n|         ABBOT Emily|           Australia|Rhythmic Gymnastics|\n|       ABBOTT Monica|United States of ...|  Baseball/Softball|\n|ABDALLA Abubaker ...|               Qatar|          Athletics|\n|      ABDALLA Maryam|               Egypt|  Artistic Swimming|\n|      ABDALLAH Shahd|               Egypt|  Artistic Swimming|\n| ABDALRASOOL Mohamed|               Sudan|               Judo|\n|   ABDEL LATIF Radwa|               Egypt|           Shooting|\n|    ABDEL RAZEK Samy|               Egypt|           Shooting|\n|   ABDELAZIZ Abdalla|               Egypt|             Karate|\n+--------------------+--------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# 8. Consistency Check\n",
    "def perform_consistency_checks(df, column, df_name):\n",
    "    inconsistent_records = df.filter(col(column) != lower(col(column)))\n",
    "    inconsistent_records.show()\n",
    "    return inconsistent_records\n",
    "\n",
    "# Example usage for checking inconsistent athlete names\n",
    "inconsistent_athlete_names = perform_consistency_checks(athletes_df, \"Name\", \"athletes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "929fb19c-a36d-4d70-8948-8c7b7f74a43d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-----+\n|Discipline|Female|Male|Total|\n+----------+------+----+-----+\n| Athletics|   969|1072| 2041|\n|  Football|   264| 344|  608|\n|  Swimming|   361| 418|  779|\n+----------+------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Outlier Detection\n",
    "def detect_outliers(df, column, df_name):\n",
    "    stats = df.selectExpr(f\"percentile({column}, 0.25) as q1\", f\"percentile({column}, 0.75) as q3\").collect()\n",
    "    q1, q3 = stats[0][\"q1\"], stats[0][\"q3\"]\n",
    "    iqr = q3 - q1\n",
    "    lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    outliers = df.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "    outliers.show()\n",
    "    return outliers\n",
    "\n",
    "outliers_total = detect_outliers(entriesgender_df, \"Total\", \"entriesgender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99c74ab-e31a-4da3-90e6-d5fc31e02479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name in athletes is within acceptable null percentage.\n"
     ]
    }
   ],
   "source": [
    "# 10. Null Percentage Check\n",
    "def null_percentage_check(df, column, threshold, df_name):\n",
    "    total_count = df.count()\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    null_percentage = (null_count / total_count) * 100\n",
    "    if null_percentage > threshold:\n",
    "        print(f\"Column {column} in {df_name} exceeds {threshold}% null values.\")\n",
    "    else:\n",
    "        print(f\"Column {column} in {df_name} is within acceptable null percentage.\")\n",
    "    return null_percentage\n",
    "\n",
    "null_percentage_total = null_percentage_check(athletes_df, \"Name\", 5, \"athletes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ccf85e-8e44-4c5f-a1c9-f3d21aceee8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----+\n|   Table|         Issue|Count|\n+--------+--------------+-----+\n|Athletes|Missing Values|    0|\n|Athletes|    Duplicates|    1|\n|  Medals|Missing Values|    0|\n|  Medals| Negative Gold|    0|\n+--------+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Consolidating quality metrics into a DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Ensure Spark session is active\n",
    "spark = SparkSession.builder.appName(\"Data Quality\").getOrCreate()\n",
    "\n",
    "quality_metrics_data = [\n",
    "    (\"Athletes\", \"Missing Values\", athletes_df.filter(athletes_df['Name'].isNull()).count()),\n",
    "    (\"Athletes\", \"Duplicates\", athletes_df.groupBy([\"Name\", \"NOC\", \"Discipline\"]).count().filter(col(\"count\") > 1).count()),\n",
    "    (\"Medals\", \"Missing Values\", medals_df.filter(medals_df['Team/NOC'].isNull()).count()),\n",
    "    (\"Medals\", \"Negative Gold\", medals_df.filter(medals_df['Gold'] < 0).count()),\n",
    "    # Add more metrics here as needed\n",
    "]\n",
    "\n",
    "# Create DataFrame for metrics\n",
    "quality_metrics_df = spark.createDataFrame(quality_metrics_data, [\"Table\", \"Issue\", \"Count\"])\n",
    "\n",
    "# Display for validation\n",
    "quality_metrics_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5e944f-9703-48ef-b79a-6f7be25d5322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define path for consolidated metrics\n",
    "consolidated_metrics_path = \"abfss://tokyo-olympic-data@tokyoolympicdata99.dfs.core.windows.net/quality-metrics/consolidated_metrics.csv\"\n",
    "\n",
    "\n",
    "# Save metrics DataFrame as CSV\n",
    "quality_metrics_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(consolidated_metrics_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10d651f-1822-4a3e-9a35-405b7cb848b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 2 Result:  None\n"
     ]
    }
   ],
   "source": [
    "# Trigger Notebook 2\n",
    "result = dbutils.notebook.run(\"Store_Quality_Metrics\", 300, {\n",
    "    \"metrics_path\": consolidated_metrics_path,\n",
    "    \"trigger_flag\": \"true\"\n",
    "})\n",
    "print(\"Notebook 2 Result: \", result)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Quality_Checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}